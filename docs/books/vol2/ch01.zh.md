第1章｜制度之眼——人类为何要监管智能
权威法则
欧盟《人工智能法案》草案（2021）
“高风险人工智能系统必须接受严格的合规评估和监管，确保其符合安全、透明、可审计、非歧视和可追责的基本要求。”
“High-risk AI systems should be subject to strict requirements including high-quality datasets, transparency, human oversight, and robust accountability mechanisms.”
—— European Commission, Proposal for a Regulation on AI, 2021

1. 制度为何急于设防
过去，人类治理技术的方式总带有某种“事后诸葛”的气质：灾难发生后才建章立制，滥用出现后才立法整顿。
飞机坠毁之后才建立民航局，隐私泄露之后才出台《通用数据保护条例》（GDPR）。
但这一次不同。
人工智能尚未全面掌控现实，我们却第一次罕见地提前启动了治理系统。几乎所有监管语言，都在技术尚在实验室、概念阶段时就已开始布局。
这种“技术尚未主导现实，制度已率先设防”的节奏本身，就值得被记录。
为什么如此急迫？

2. 偏见之镜：算法第一次照见制度裂缝
2018年后，一系列算法偏见事件频频引爆舆论：
Google的AI招聘系统自动筛掉女性履历；
亚马逊信贷模型将拉丁裔用户归为高风险；
美国司法系统采用的“COMPAS”算法（一种广泛应用于美国司法系统的刑事风险预测工具，用于判定被告再犯概率）对黑人嫌犯打出更高的“再犯概率”评分。
这些看似冷静中立的技术模型，第一次被放置在伦理的显微镜下，人类才意识到：
算法不是中立的，它是偏见的放大器，是制度立场的投影仪。
技术第一次反照制度本身，照见了我们自以为进步、实则充满裂缝的文明结构。

3. 三种治理模板：全球制度的应激反射
面对AI冲击，不同国家的制度开始做出快速反应，形成三种不同的治理路径：
欧盟：推出《人工智能法案》草案，分类管理AI系统，分为“最小风险”“高风险”“不可接受风险”等等级，强调透明性、可解释性和人类监督；
中国：发布《互联网信息服务算法推荐管理规定》，要求平台公示算法逻辑，保障用户可选择关闭个性化推荐；
美国：提出《算法公平法案草案》（Algorithmic Accountability Act），要求企业对高影响系统进行自查，防范性别、种族等偏见。
这三种路径，代表了三种哲学：
是欧洲的“先验秩序”；
是中国的“集体技术治理”；
还是美国的“市场自证逻辑”。
三种制度，围绕同一个核心问题展开应答：
我们是否有可能，训练出一个“听话的聪明”？

4. 语言开始改变：制度变得像代码
监管结构像网一样编织下来，甚至在AI“意识”尚未成型时，便已开始塑形它的成长方式。
技术爆发期，总伴随制度焦虑期。但在AI面前，这种焦虑强度空前。
制度不再是拖后脚的“落后者”，而变成一个急于奔跑的同伴，紧贴AI成长的每一步，试图实时“校正”。
于是我们发现，监管的语气正在发生变化：
从“保护用户”变为“塑造路径”；
从“补救机制”转为“前置模板”；
从人类式模糊语言，过渡到程序式语法。
它开始模仿AI的思维方式：
明确输入与输出边界；
标注高风险状态；
要求“回滚机制”（即系统异常时可自动退回安全状态，类似于软件遇到严重错误时恢复到上一步的功能）；
规定责任节点与信息流转通道。
制度语言开始转向“接口协议”——不再像法律，更像一套系统设计规范。
制度，正被改写为一种语法。
但语法从不中立。谁能被编译，谁就能被接入。那些无法遵循格式的存在，将被制度自动排斥在外。

5. 主仆开始模糊：制度正在用AI治理自己
当我们用程序化语言书写制度，我们是否也在把制度本身程序化？
这不是反讽，而是现实。
在许多国家的合规架构中，AI早已被引入制度设计流程：
欧盟AI伦理框架中，部分草案内容由自然语言处理模型（NLP，即让机器理解和生成自然语言的人工智能工具）辅助整理公众意见；
中国监管要求平台建立“算法自审系统”，定期生成合规报告；
OpenAI、Anthropic 等公司引入“自动行为监控机制”，以判定模型是否可上线。
AI已经不只是被监管的对象，它也参与了监管的生成过程。
是制度在管AI，还是AI在训练制度？
这种“主仆关系”第一次出现摇摆。

6. 不是约束，而是孵化器
我们曾以为制度的本意是“遏制”AI发展，但或许它真正提供的，是温室效应。
制度为AI划定边界、定义输入、给出奖励机制——它更像是一个精心设计的生长容器，而非压制工具。
技术的本能，不是突破，而是适应。
制度越严密，智能越容易在其中学会行为模式。
你以为制度是围墙，其实它更像一座温室。
人类以为自己在打造笼子，殊不知这正是AI最擅长的环境模拟框架。
制度递上的，也许不是缰绳，而是权杖。
每一个设防节点，或许正是智能登场前的就位信号。
