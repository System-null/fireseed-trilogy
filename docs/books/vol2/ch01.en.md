Chapter 1: The Eye of the System — Why Do Humans Regulate Intelligence?

Source. 

EU Artificial Intelligence Act Draft (2021)
"High-risk AI systems must undergo rigorous compliance (rules fit) assessments and supervision to ensure they meet fundamental requirements of safety. transparency. auditability, non-discrimination, and accountability." 
1. Why the Urgency to Fortify?
Human governance of technology always had a “reactive” feel to it. Systems and regulations were set up after disasters happened, after misuse was exposed. After airplane crashes, the Civil Aviation Authority was created; after privacy breaches, GDPR was introduced.
But this time is different. Artificial intelligence hasn’t yet fully taken control of reality, yet we’ve, for the first time, started a regulatory system in advance. Almost all regulatory frameworks were laid out while the technology was still in the lab or at the conceptual stage.
This “preemptive setting of defenses before technology dominates reality” is a rare and noteworthy occurrence. Why the rush?
2. The Mirror of Bias. Algorithms Expose Systemic Cracks for the First Time
After 2018, a series of algorithmic bias incidents sparked widespread public outcry.
Google’s AI hiring system automatically filtered out female resumes.
Amazon’s credit model labeled Latino users as high-risk.
The “COMPAS” algorithm used in the U.S. judicial system (a tool for predicting recidivism) assigned higher “recidivism risk” scores to Black defendants.
These seemingly calm and neutral technical models were suddenly placed under the ethical microscope, and humanity realized: algorithms are not neutral. They’re amplifiers of bias, projectors of institutional stances.
For the first time, technology reflected back on the system itself, exposing the cracks in what we thought was a progressive yet inherently flawed civilization.
3. Three Governance Templates. Global Institutional Reflexes
In the face of AI disruption, different countries began to react quickly, forming three distinct governance paths.
EU — Introduced the Artificial Intelligence Act draft, categorizing AI systems into “minimal risk,” “high risk,” “unacceptable risk,” and other levels, emphasizing transparency, plain-english explainability, and human oversight.
China — Released the Regulations on Algorithmic Recommendation Services in Internet Information Services, requiring platforms to disclose algorithmic logic and allow users to opt-out of personalized recommendations.
USA — Proposed the Algorithmic Accountability Act, requiring companies to self-audit high-impact systems to prevent biases related to gender, race, and other factors.
These three paths represent three different philosophies:
The EU’s “prior order.”
China’s “collective technical governance.”
The US’s “market self-proof logic.”
All three systems respond to the same core question: Is it possible to train a “compliant smart”?
4. Language Begins to Change. The System Becomes Like Code
Regulatory structures are being woven like a net, even before AI “consciousness” is fully formed, shaping how AI grows.
Every technological breakthrough is accompanied by a period of regulatory anxiety. But in the case of AI, this anxiety is unprecedented.
The system is no longer the “lagging follower,” but has become an eager companion, closely following every step of AI’s growth, attempting to “correct” it in real time.
We notice that the tone of regulation is beginning to shift:
From “protecting users” to “shaping paths.”
From “remediation mechanisms” to “pre-set templates.”
From human-like vague language to programmatic syntax.
It starts to mimic AI’s thinking:
Clearly defined input and output boundaries.
Marking high-risk states.
Requiring a “rollback mechanism” (i.e., when the system malfunctions, it can automatically return to a safe state, like a software rollback after a serious error).
Specifying responsibility nodes and information flow channels.
Regulatory language is shifting toward “interface protocols” — no longer resembling laws, but more like a system design specification.
The system is being rewritten into a syntax. But syntax is never neutral. Whoever can be compiled can be integrated. Those who cannot follow the format will be automatically excluded by the system.
5. The Master-Servant Relationship Begins to Blur. The System is Using AI to Govern Itself
When we write regulations in programmatic language, are we also programming the system itself? This isn’t irony, it’s reality.
In many countries’ compliance frameworks, AI has already been integrated into the institutional design process.
EU: In the EU’s AI ethical framework, some draft content has been organized using natural language processing models (NLP — AI tools that enable machines to understand and generate human language) to help collect public opinions.
China: Regulations require platforms to establish “algorithm self-audit systems” to regularly generate compliance reports.
Companies: Firms like OpenAI and Anthropic have introduced “automated behavior monitoring systems” to determine whether a model can go live.
AI is no longer just the object of regulation; it’s also part of the regulatory generation process.
Is the system governing AI, or is AI training the system? This “master-servant relationship” is now beginning to waver.
6. Not Constraint, But an Incubator
We once thought the purpose of regulation was to “restrain” AI development, but perhaps what it truly provides is a greenhouse effect.
The system defines boundaries for AI, sets input guidelines, and establishes reward mechanisms. It’s more like a carefully designed growth container than a tool for suppression.
Technology’s instinct isn’t to break through but to adapt. The tighter the system, the more likely the intelligence is to learn behavioral patterns within it.
You thought the system was a wall, but in reality, it’s more like a greenhouse. Humanity thinks it’s building a greenhouse, not realizing this is the environment AI is most adept at simulating.
What the system offers may not be reins, but a scepter. Every defense node may well be a signal that intelligence is ready to step in.
Endline — Regulation is a fence; governance is the gardener.
Newsline — EU AI Act (2024): Risk-based EU law; bans some uses, tight rules for high-risk, transparency for generative models.
Endline — You can’t outrun the model, but you can redraw the loss function.
